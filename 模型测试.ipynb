{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from functools import partial\n",
    "from collections import OrderedDict\n",
    "from torchsummary import summary\n",
    "from tensorboardX import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\XK\\AppData\\Local\\Temp\\ipykernel_16996\\3788881458.py:19: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  omega = np.arange(embed_dim // 2, dtype=np.float)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 224, 224])\n",
      "torch.Size([2, 14, 14, 16, 16, 3])\n",
      "torch.Size([2, 196, 768])\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [1, 768, 14, 14]         590,592\n",
      "          Identity-2              [1, 196, 768]               0\n",
      "        PatchEmbed-3              [1, 196, 768]               0\n",
      "         LayerNorm-4               [1, 50, 768]           1,536\n",
      "            Linear-5              [1, 50, 2304]       1,771,776\n",
      "           Dropout-6            [1, 12, 50, 50]               0\n",
      "            Linear-7               [1, 50, 768]         590,592\n",
      "           Dropout-8               [1, 50, 768]               0\n",
      "         Attention-9               [1, 50, 768]               0\n",
      "         Identity-10               [1, 50, 768]               0\n",
      "        LayerNorm-11               [1, 50, 768]           1,536\n",
      "           Linear-12              [1, 50, 3072]       2,362,368\n",
      "             GELU-13              [1, 50, 3072]               0\n",
      "          Dropout-14              [1, 50, 3072]               0\n",
      "           Linear-15               [1, 50, 768]       2,360,064\n",
      "          Dropout-16               [1, 50, 768]               0\n",
      "              Mlp-17               [1, 50, 768]               0\n",
      "         Identity-18               [1, 50, 768]               0\n",
      "            Block-19               [1, 50, 768]               0\n",
      "        LayerNorm-20               [1, 50, 768]           1,536\n",
      "           Linear-21              [1, 50, 2304]       1,771,776\n",
      "          Dropout-22            [1, 12, 50, 50]               0\n",
      "           Linear-23               [1, 50, 768]         590,592\n",
      "          Dropout-24               [1, 50, 768]               0\n",
      "        Attention-25               [1, 50, 768]               0\n",
      "         Identity-26               [1, 50, 768]               0\n",
      "        LayerNorm-27               [1, 50, 768]           1,536\n",
      "           Linear-28              [1, 50, 3072]       2,362,368\n",
      "             GELU-29              [1, 50, 3072]               0\n",
      "          Dropout-30              [1, 50, 3072]               0\n",
      "           Linear-31               [1, 50, 768]       2,360,064\n",
      "          Dropout-32               [1, 50, 768]               0\n",
      "              Mlp-33               [1, 50, 768]               0\n",
      "         Identity-34               [1, 50, 768]               0\n",
      "            Block-35               [1, 50, 768]               0\n",
      "        LayerNorm-36               [1, 50, 768]           1,536\n",
      "           Linear-37              [1, 50, 2304]       1,771,776\n",
      "          Dropout-38            [1, 12, 50, 50]               0\n",
      "           Linear-39               [1, 50, 768]         590,592\n",
      "          Dropout-40               [1, 50, 768]               0\n",
      "        Attention-41               [1, 50, 768]               0\n",
      "         Identity-42               [1, 50, 768]               0\n",
      "        LayerNorm-43               [1, 50, 768]           1,536\n",
      "           Linear-44              [1, 50, 3072]       2,362,368\n",
      "             GELU-45              [1, 50, 3072]               0\n",
      "          Dropout-46              [1, 50, 3072]               0\n",
      "           Linear-47               [1, 50, 768]       2,360,064\n",
      "          Dropout-48               [1, 50, 768]               0\n",
      "              Mlp-49               [1, 50, 768]               0\n",
      "         Identity-50               [1, 50, 768]               0\n",
      "            Block-51               [1, 50, 768]               0\n",
      "        LayerNorm-52               [1, 50, 768]           1,536\n",
      "           Linear-53              [1, 50, 2304]       1,771,776\n",
      "          Dropout-54            [1, 12, 50, 50]               0\n",
      "           Linear-55               [1, 50, 768]         590,592\n",
      "          Dropout-56               [1, 50, 768]               0\n",
      "        Attention-57               [1, 50, 768]               0\n",
      "         Identity-58               [1, 50, 768]               0\n",
      "        LayerNorm-59               [1, 50, 768]           1,536\n",
      "           Linear-60              [1, 50, 3072]       2,362,368\n",
      "             GELU-61              [1, 50, 3072]               0\n",
      "          Dropout-62              [1, 50, 3072]               0\n",
      "           Linear-63               [1, 50, 768]       2,360,064\n",
      "          Dropout-64               [1, 50, 768]               0\n",
      "              Mlp-65               [1, 50, 768]               0\n",
      "         Identity-66               [1, 50, 768]               0\n",
      "            Block-67               [1, 50, 768]               0\n",
      "        LayerNorm-68               [1, 50, 768]           1,536\n",
      "           Linear-69              [1, 50, 2304]       1,771,776\n",
      "          Dropout-70            [1, 12, 50, 50]               0\n",
      "           Linear-71               [1, 50, 768]         590,592\n",
      "          Dropout-72               [1, 50, 768]               0\n",
      "        Attention-73               [1, 50, 768]               0\n",
      "         Identity-74               [1, 50, 768]               0\n",
      "        LayerNorm-75               [1, 50, 768]           1,536\n",
      "           Linear-76              [1, 50, 3072]       2,362,368\n",
      "             GELU-77              [1, 50, 3072]               0\n",
      "          Dropout-78              [1, 50, 3072]               0\n",
      "           Linear-79               [1, 50, 768]       2,360,064\n",
      "          Dropout-80               [1, 50, 768]               0\n",
      "              Mlp-81               [1, 50, 768]               0\n",
      "         Identity-82               [1, 50, 768]               0\n",
      "            Block-83               [1, 50, 768]               0\n",
      "        LayerNorm-84               [1, 50, 768]           1,536\n",
      "           Linear-85              [1, 50, 2304]       1,771,776\n",
      "          Dropout-86            [1, 12, 50, 50]               0\n",
      "           Linear-87               [1, 50, 768]         590,592\n",
      "          Dropout-88               [1, 50, 768]               0\n",
      "        Attention-89               [1, 50, 768]               0\n",
      "         Identity-90               [1, 50, 768]               0\n",
      "        LayerNorm-91               [1, 50, 768]           1,536\n",
      "           Linear-92              [1, 50, 3072]       2,362,368\n",
      "             GELU-93              [1, 50, 3072]               0\n",
      "          Dropout-94              [1, 50, 3072]               0\n",
      "           Linear-95               [1, 50, 768]       2,360,064\n",
      "          Dropout-96               [1, 50, 768]               0\n",
      "              Mlp-97               [1, 50, 768]               0\n",
      "         Identity-98               [1, 50, 768]               0\n",
      "            Block-99               [1, 50, 768]               0\n",
      "       LayerNorm-100               [1, 50, 768]           1,536\n",
      "          Linear-101              [1, 50, 2304]       1,771,776\n",
      "         Dropout-102            [1, 12, 50, 50]               0\n",
      "          Linear-103               [1, 50, 768]         590,592\n",
      "         Dropout-104               [1, 50, 768]               0\n",
      "       Attention-105               [1, 50, 768]               0\n",
      "        Identity-106               [1, 50, 768]               0\n",
      "       LayerNorm-107               [1, 50, 768]           1,536\n",
      "          Linear-108              [1, 50, 3072]       2,362,368\n",
      "            GELU-109              [1, 50, 3072]               0\n",
      "         Dropout-110              [1, 50, 3072]               0\n",
      "          Linear-111               [1, 50, 768]       2,360,064\n",
      "         Dropout-112               [1, 50, 768]               0\n",
      "             Mlp-113               [1, 50, 768]               0\n",
      "        Identity-114               [1, 50, 768]               0\n",
      "           Block-115               [1, 50, 768]               0\n",
      "       LayerNorm-116               [1, 50, 768]           1,536\n",
      "          Linear-117              [1, 50, 2304]       1,771,776\n",
      "         Dropout-118            [1, 12, 50, 50]               0\n",
      "          Linear-119               [1, 50, 768]         590,592\n",
      "         Dropout-120               [1, 50, 768]               0\n",
      "       Attention-121               [1, 50, 768]               0\n",
      "        Identity-122               [1, 50, 768]               0\n",
      "       LayerNorm-123               [1, 50, 768]           1,536\n",
      "          Linear-124              [1, 50, 3072]       2,362,368\n",
      "            GELU-125              [1, 50, 3072]               0\n",
      "         Dropout-126              [1, 50, 3072]               0\n",
      "          Linear-127               [1, 50, 768]       2,360,064\n",
      "         Dropout-128               [1, 50, 768]               0\n",
      "             Mlp-129               [1, 50, 768]               0\n",
      "        Identity-130               [1, 50, 768]               0\n",
      "           Block-131               [1, 50, 768]               0\n",
      "       LayerNorm-132               [1, 50, 768]           1,536\n",
      "          Linear-133              [1, 50, 2304]       1,771,776\n",
      "         Dropout-134            [1, 12, 50, 50]               0\n",
      "          Linear-135               [1, 50, 768]         590,592\n",
      "         Dropout-136               [1, 50, 768]               0\n",
      "       Attention-137               [1, 50, 768]               0\n",
      "        Identity-138               [1, 50, 768]               0\n",
      "       LayerNorm-139               [1, 50, 768]           1,536\n",
      "          Linear-140              [1, 50, 3072]       2,362,368\n",
      "            GELU-141              [1, 50, 3072]               0\n",
      "         Dropout-142              [1, 50, 3072]               0\n",
      "          Linear-143               [1, 50, 768]       2,360,064\n",
      "         Dropout-144               [1, 50, 768]               0\n",
      "             Mlp-145               [1, 50, 768]               0\n",
      "        Identity-146               [1, 50, 768]               0\n",
      "           Block-147               [1, 50, 768]               0\n",
      "       LayerNorm-148               [1, 50, 768]           1,536\n",
      "          Linear-149              [1, 50, 2304]       1,771,776\n",
      "         Dropout-150            [1, 12, 50, 50]               0\n",
      "          Linear-151               [1, 50, 768]         590,592\n",
      "         Dropout-152               [1, 50, 768]               0\n",
      "       Attention-153               [1, 50, 768]               0\n",
      "        Identity-154               [1, 50, 768]               0\n",
      "       LayerNorm-155               [1, 50, 768]           1,536\n",
      "          Linear-156              [1, 50, 3072]       2,362,368\n",
      "            GELU-157              [1, 50, 3072]               0\n",
      "         Dropout-158              [1, 50, 3072]               0\n",
      "          Linear-159               [1, 50, 768]       2,360,064\n",
      "         Dropout-160               [1, 50, 768]               0\n",
      "             Mlp-161               [1, 50, 768]               0\n",
      "        Identity-162               [1, 50, 768]               0\n",
      "           Block-163               [1, 50, 768]               0\n",
      "       LayerNorm-164               [1, 50, 768]           1,536\n",
      "          Linear-165              [1, 50, 2304]       1,771,776\n",
      "         Dropout-166            [1, 12, 50, 50]               0\n",
      "          Linear-167               [1, 50, 768]         590,592\n",
      "         Dropout-168               [1, 50, 768]               0\n",
      "       Attention-169               [1, 50, 768]               0\n",
      "        Identity-170               [1, 50, 768]               0\n",
      "       LayerNorm-171               [1, 50, 768]           1,536\n",
      "          Linear-172              [1, 50, 3072]       2,362,368\n",
      "            GELU-173              [1, 50, 3072]               0\n",
      "         Dropout-174              [1, 50, 3072]               0\n",
      "          Linear-175               [1, 50, 768]       2,360,064\n",
      "         Dropout-176               [1, 50, 768]               0\n",
      "             Mlp-177               [1, 50, 768]               0\n",
      "        Identity-178               [1, 50, 768]               0\n",
      "           Block-179               [1, 50, 768]               0\n",
      "       LayerNorm-180               [1, 50, 768]           1,536\n",
      "          Linear-181              [1, 50, 2304]       1,771,776\n",
      "         Dropout-182            [1, 12, 50, 50]               0\n",
      "          Linear-183               [1, 50, 768]         590,592\n",
      "         Dropout-184               [1, 50, 768]               0\n",
      "       Attention-185               [1, 50, 768]               0\n",
      "        Identity-186               [1, 50, 768]               0\n",
      "       LayerNorm-187               [1, 50, 768]           1,536\n",
      "          Linear-188              [1, 50, 3072]       2,362,368\n",
      "            GELU-189              [1, 50, 3072]               0\n",
      "         Dropout-190              [1, 50, 3072]               0\n",
      "          Linear-191               [1, 50, 768]       2,360,064\n",
      "         Dropout-192               [1, 50, 768]               0\n",
      "             Mlp-193               [1, 50, 768]               0\n",
      "        Identity-194               [1, 50, 768]               0\n",
      "           Block-195               [1, 50, 768]               0\n",
      "       LayerNorm-196               [1, 50, 768]           1,536\n",
      "          Linear-197               [1, 50, 512]         393,728\n",
      "       LayerNorm-198              [1, 197, 512]           1,024\n",
      "          Linear-199             [1, 197, 1536]         787,968\n",
      "         Dropout-200          [1, 16, 197, 197]               0\n",
      "          Linear-201              [1, 197, 512]         262,656\n",
      "         Dropout-202              [1, 197, 512]               0\n",
      "       Attention-203              [1, 197, 512]               0\n",
      "        Identity-204              [1, 197, 512]               0\n",
      "       LayerNorm-205              [1, 197, 512]           1,024\n",
      "          Linear-206             [1, 197, 2048]       1,050,624\n",
      "            GELU-207             [1, 197, 2048]               0\n",
      "         Dropout-208             [1, 197, 2048]               0\n",
      "          Linear-209              [1, 197, 512]       1,049,088\n",
      "         Dropout-210              [1, 197, 512]               0\n",
      "             Mlp-211              [1, 197, 512]               0\n",
      "        Identity-212              [1, 197, 512]               0\n",
      "           Block-213              [1, 197, 512]               0\n",
      "       LayerNorm-214              [1, 197, 512]           1,024\n",
      "          Linear-215             [1, 197, 1536]         787,968\n",
      "         Dropout-216          [1, 16, 197, 197]               0\n",
      "          Linear-217              [1, 197, 512]         262,656\n",
      "         Dropout-218              [1, 197, 512]               0\n",
      "       Attention-219              [1, 197, 512]               0\n",
      "        Identity-220              [1, 197, 512]               0\n",
      "       LayerNorm-221              [1, 197, 512]           1,024\n",
      "          Linear-222             [1, 197, 2048]       1,050,624\n",
      "            GELU-223             [1, 197, 2048]               0\n",
      "         Dropout-224             [1, 197, 2048]               0\n",
      "          Linear-225              [1, 197, 512]       1,049,088\n",
      "         Dropout-226              [1, 197, 512]               0\n",
      "             Mlp-227              [1, 197, 512]               0\n",
      "        Identity-228              [1, 197, 512]               0\n",
      "           Block-229              [1, 197, 512]               0\n",
      "       LayerNorm-230              [1, 197, 512]           1,024\n",
      "          Linear-231             [1, 197, 1536]         787,968\n",
      "         Dropout-232          [1, 16, 197, 197]               0\n",
      "          Linear-233              [1, 197, 512]         262,656\n",
      "         Dropout-234              [1, 197, 512]               0\n",
      "       Attention-235              [1, 197, 512]               0\n",
      "        Identity-236              [1, 197, 512]               0\n",
      "       LayerNorm-237              [1, 197, 512]           1,024\n",
      "          Linear-238             [1, 197, 2048]       1,050,624\n",
      "            GELU-239             [1, 197, 2048]               0\n",
      "         Dropout-240             [1, 197, 2048]               0\n",
      "          Linear-241              [1, 197, 512]       1,049,088\n",
      "         Dropout-242              [1, 197, 512]               0\n",
      "             Mlp-243              [1, 197, 512]               0\n",
      "        Identity-244              [1, 197, 512]               0\n",
      "           Block-245              [1, 197, 512]               0\n",
      "       LayerNorm-246              [1, 197, 512]           1,024\n",
      "          Linear-247             [1, 197, 1536]         787,968\n",
      "         Dropout-248          [1, 16, 197, 197]               0\n",
      "          Linear-249              [1, 197, 512]         262,656\n",
      "         Dropout-250              [1, 197, 512]               0\n",
      "       Attention-251              [1, 197, 512]               0\n",
      "        Identity-252              [1, 197, 512]               0\n",
      "       LayerNorm-253              [1, 197, 512]           1,024\n",
      "          Linear-254             [1, 197, 2048]       1,050,624\n",
      "            GELU-255             [1, 197, 2048]               0\n",
      "         Dropout-256             [1, 197, 2048]               0\n",
      "          Linear-257              [1, 197, 512]       1,049,088\n",
      "         Dropout-258              [1, 197, 512]               0\n",
      "             Mlp-259              [1, 197, 512]               0\n",
      "        Identity-260              [1, 197, 512]               0\n",
      "           Block-261              [1, 197, 512]               0\n",
      "       LayerNorm-262              [1, 197, 512]           1,024\n",
      "          Linear-263             [1, 197, 1536]         787,968\n",
      "         Dropout-264          [1, 16, 197, 197]               0\n",
      "          Linear-265              [1, 197, 512]         262,656\n",
      "         Dropout-266              [1, 197, 512]               0\n",
      "       Attention-267              [1, 197, 512]               0\n",
      "        Identity-268              [1, 197, 512]               0\n",
      "       LayerNorm-269              [1, 197, 512]           1,024\n",
      "          Linear-270             [1, 197, 2048]       1,050,624\n",
      "            GELU-271             [1, 197, 2048]               0\n",
      "         Dropout-272             [1, 197, 2048]               0\n",
      "          Linear-273              [1, 197, 512]       1,049,088\n",
      "         Dropout-274              [1, 197, 512]               0\n",
      "             Mlp-275              [1, 197, 512]               0\n",
      "        Identity-276              [1, 197, 512]               0\n",
      "           Block-277              [1, 197, 512]               0\n",
      "       LayerNorm-278              [1, 197, 512]           1,024\n",
      "          Linear-279             [1, 197, 1536]         787,968\n",
      "         Dropout-280          [1, 16, 197, 197]               0\n",
      "          Linear-281              [1, 197, 512]         262,656\n",
      "         Dropout-282              [1, 197, 512]               0\n",
      "       Attention-283              [1, 197, 512]               0\n",
      "        Identity-284              [1, 197, 512]               0\n",
      "       LayerNorm-285              [1, 197, 512]           1,024\n",
      "          Linear-286             [1, 197, 2048]       1,050,624\n",
      "            GELU-287             [1, 197, 2048]               0\n",
      "         Dropout-288             [1, 197, 2048]               0\n",
      "          Linear-289              [1, 197, 512]       1,049,088\n",
      "         Dropout-290              [1, 197, 512]               0\n",
      "             Mlp-291              [1, 197, 512]               0\n",
      "        Identity-292              [1, 197, 512]               0\n",
      "           Block-293              [1, 197, 512]               0\n",
      "       LayerNorm-294              [1, 197, 512]           1,024\n",
      "          Linear-295             [1, 197, 1536]         787,968\n",
      "         Dropout-296          [1, 16, 197, 197]               0\n",
      "          Linear-297              [1, 197, 512]         262,656\n",
      "         Dropout-298              [1, 197, 512]               0\n",
      "       Attention-299              [1, 197, 512]               0\n",
      "        Identity-300              [1, 197, 512]               0\n",
      "       LayerNorm-301              [1, 197, 512]           1,024\n",
      "          Linear-302             [1, 197, 2048]       1,050,624\n",
      "            GELU-303             [1, 197, 2048]               0\n",
      "         Dropout-304             [1, 197, 2048]               0\n",
      "          Linear-305              [1, 197, 512]       1,049,088\n",
      "         Dropout-306              [1, 197, 512]               0\n",
      "             Mlp-307              [1, 197, 512]               0\n",
      "        Identity-308              [1, 197, 512]               0\n",
      "           Block-309              [1, 197, 512]               0\n",
      "       LayerNorm-310              [1, 197, 512]           1,024\n",
      "          Linear-311             [1, 197, 1536]         787,968\n",
      "         Dropout-312          [1, 16, 197, 197]               0\n",
      "          Linear-313              [1, 197, 512]         262,656\n",
      "         Dropout-314              [1, 197, 512]               0\n",
      "       Attention-315              [1, 197, 512]               0\n",
      "        Identity-316              [1, 197, 512]               0\n",
      "       LayerNorm-317              [1, 197, 512]           1,024\n",
      "          Linear-318             [1, 197, 2048]       1,050,624\n",
      "            GELU-319             [1, 197, 2048]               0\n",
      "         Dropout-320             [1, 197, 2048]               0\n",
      "          Linear-321              [1, 197, 512]       1,049,088\n",
      "         Dropout-322              [1, 197, 512]               0\n",
      "             Mlp-323              [1, 197, 512]               0\n",
      "        Identity-324              [1, 197, 512]               0\n",
      "           Block-325              [1, 197, 512]               0\n",
      "       LayerNorm-326              [1, 197, 512]           1,024\n",
      "          Linear-327              [1, 197, 768]         393,984\n",
      "================================================================\n",
      "Total params: 111,654,400\n",
      "Trainable params: 111,654,400\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 297.97\n",
      "Params size (MB): 425.93\n",
      "Estimated Total Size (MB): 724.47\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def get_2d_sincos_pos_embed_from_grid(embed_dim, grid):\n",
    "    assert embed_dim % 2 == 0\n",
    "\n",
    "    # use half of dimensions to encode grid_h\n",
    "    emb_h = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[0])  # (H*W, D/2)\n",
    "    emb_w = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[1])  # (H*W, D/2)\n",
    "\n",
    "    emb = np.concatenate([emb_h, emb_w], axis=1) # (H*W, D)\n",
    "    return emb\n",
    "\n",
    "\n",
    "def get_1d_sincos_pos_embed_from_grid(embed_dim, pos):\n",
    "    \"\"\"\n",
    "    embed_dim: output dimension for each position\n",
    "    pos: a list of positions to be encoded: size (M,)\n",
    "    out: (M, D)\n",
    "    \"\"\"\n",
    "    assert embed_dim % 2 == 0\n",
    "    omega = np.arange(embed_dim // 2, dtype=np.float)\n",
    "    omega /= embed_dim / 2.\n",
    "    omega = 1. / 10000**omega  # (D/2,)\n",
    "\n",
    "    pos = pos.reshape(-1)  # (M,)\n",
    "    out = np.einsum('m,d->md', pos, omega)  # (M, D/2), outer product\n",
    "\n",
    "    emb_sin = np.sin(out) # (M, D/2)\n",
    "    emb_cos = np.cos(out) # (M, D/2)\n",
    "\n",
    "    emb = np.concatenate([emb_sin, emb_cos], axis=1)  # (M, D)\n",
    "    return emb\n",
    "\n",
    "\n",
    "def get_2d_sincos_pos_embed(embed_dim, grid_size, cls_token=False):\n",
    "    \"\"\"\n",
    "    grid_size: int of the grid height and width\n",
    "    return:\n",
    "    pos_embed: [grid_size*grid_size, embed_dim] or [1+grid_size*grid_size, embed_dim] (w/ or w/o cls_token)\n",
    "    \"\"\"\n",
    "    grid_h = np.arange(grid_size, dtype=np.float32)\n",
    "    grid_w = np.arange(grid_size, dtype=np.float32)\n",
    "    grid = np.meshgrid(grid_w, grid_h)  # here w goes first\n",
    "    grid = np.stack(grid, axis=0)\n",
    "\n",
    "    grid = grid.reshape([2, 1, grid_size, grid_size])\n",
    "    pos_embed = get_2d_sincos_pos_embed_from_grid(embed_dim, grid)\n",
    "    if cls_token:\n",
    "        pos_embed = np.concatenate([np.zeros([1, embed_dim]), pos_embed], axis=0)\n",
    "    return pos_embed\n",
    "\n",
    "from timm.models.vision_transformer import PatchEmbed, Block\n",
    "\n",
    "class 掩码自编码器Vit(nn.Module):\n",
    "    \"\"\" Masked Autoencoder with VisionTransformer backbone\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, img_size=224, patch_size=16, in_chans=3,\n",
    "                 embed_dim=1024, depth=24, num_heads=16,\n",
    "                 decoder_embed_dim=512, decoder_depth=8, decoder_num_heads=16,\n",
    "                 mlp_ratio=4., norm_layer=nn.LayerNorm, norm_pix_loss=False):\n",
    "        super().__init__()\n",
    "\n",
    "        # --------------------------------------------------------------------------\n",
    "        # MAE encoder specifics\n",
    "        self.patch_embed = PatchEmbed(img_size, patch_size, in_chans, embed_dim)\n",
    "        num_patches = self.patch_embed.num_patches\n",
    "\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim),\n",
    "                                      requires_grad=False)  # fixed sin-cos embedding\n",
    "\n",
    "        self.blocks = nn.ModuleList([\n",
    "            Block(embed_dim, num_heads, mlp_ratio, qkv_bias=True, norm_layer=norm_layer)\n",
    "            for i in range(depth)])\n",
    "        self.norm = norm_layer(embed_dim)\n",
    "        # --------------------------------------------------------------------------\n",
    "\n",
    "        # --------------------------------------------------------------------------\n",
    "        # MAE decoder specifics\n",
    "        self.decoder_embed = nn.Linear(embed_dim, decoder_embed_dim, bias=True)\n",
    "\n",
    "        self.mask_token = nn.Parameter(torch.zeros(1, 1, decoder_embed_dim))\n",
    "\n",
    "        self.decoder_pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, decoder_embed_dim),\n",
    "                                              requires_grad=False)  # fixed sin-cos embedding\n",
    "\n",
    "        self.decoder_blocks = nn.ModuleList([\n",
    "            Block(decoder_embed_dim, decoder_num_heads, mlp_ratio, qkv_bias=True, norm_layer=norm_layer)\n",
    "            for i in range(decoder_depth)])\n",
    "\n",
    "        self.decoder_norm = norm_layer(decoder_embed_dim)\n",
    "        self.decoder_pred = nn.Linear(decoder_embed_dim, patch_size ** 2 * in_chans, bias=True)  # decoder to patch\n",
    "        # --------------------------------------------------------------------------\n",
    "\n",
    "        self.norm_pix_loss = norm_pix_loss\n",
    "\n",
    "        self.initialize_weights()\n",
    "\n",
    "    def initialize_weights(self):\n",
    "        # initialization\n",
    "        # initialize (and freeze) pos_embed by sin-cos embedding\n",
    "        pos_embed = get_2d_sincos_pos_embed(self.pos_embed.shape[-1], int(self.patch_embed.num_patches ** .5),\n",
    "                                            cls_token=True)\n",
    "        self.pos_embed.data.copy_(torch.from_numpy(pos_embed).float().unsqueeze(0))\n",
    "\n",
    "        decoder_pos_embed = get_2d_sincos_pos_embed(self.decoder_pos_embed.shape[-1],\n",
    "                                                    int(self.patch_embed.num_patches ** .5), cls_token=True)\n",
    "        self.decoder_pos_embed.data.copy_(torch.from_numpy(decoder_pos_embed).float().unsqueeze(0))\n",
    "\n",
    "        # initialize patch_embed like nn.Linear (instead of nn.Conv2d)\n",
    "        w = self.patch_embed.proj.weight.data\n",
    "        torch.nn.init.xavier_uniform_(w.view([w.shape[0], -1]))\n",
    "\n",
    "        # timm's trunc_normal_(std=.02) is effectively normal_(std=0.02) as cutoff is too big (2.)\n",
    "        torch.nn.init.normal_(self.cls_token, std=.02)\n",
    "        torch.nn.init.normal_(self.mask_token, std=.02)\n",
    "\n",
    "        # initialize nn.Linear and nn.LayerNorm\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            # we use xavier_uniform following official JAX ViT:\n",
    "            torch.nn.init.xavier_uniform_(m.weight)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "    def patchify(self, imgs):\n",
    "        \"\"\"\n",
    "        imgs: (N, 3, H, W)\n",
    "        x: (N, L, patch_size**2 *3)\n",
    "        \"\"\"\n",
    "        p = self.patch_embed.patch_size[0]\n",
    "        assert imgs.shape[2] == imgs.shape[3] and imgs.shape[2] % p == 0\n",
    "        print(imgs.shape)\n",
    "        h = w = imgs.shape[2] // p\n",
    "        x = imgs.reshape(shape=(imgs.shape[0], 3, h, p, w, p))\n",
    "        x = torch.einsum('nchpwq->nhwpqc', x)\n",
    "        print(x.shape)\n",
    "        x = x.reshape(shape=(imgs.shape[0], h * w, p ** 2 * 3))\n",
    "        print(x.shape) # x形状[批量, 块数量, 嵌入向量维度]\n",
    "        return x\n",
    "\n",
    "    def unpatchify(self, x):\n",
    "        \"\"\"\n",
    "        x: (N, L, patch_size**2 *3)\n",
    "        imgs: (N, 3, H, W)\n",
    "        \"\"\"\n",
    "        p = self.patch_embed.patch_size[0]\n",
    "        h = w = int(x.shape[1] ** .5)\n",
    "        assert h * w == x.shape[1]\n",
    "\n",
    "        x = x.reshape(shape=(x.shape[0], h, w, p, p, 3))\n",
    "        x = torch.einsum('nhwpqc->nchpwq', x)\n",
    "        imgs = x.reshape(shape=(x.shape[0], 3, h * p, h * p))\n",
    "        return imgs\n",
    "\n",
    "    def random_masking(self, x, mask_ratio):\n",
    "        \"\"\"\n",
    "        Perform per-sample random masking by per-sample shuffling.\n",
    "        Per-sample shuffling is done by argsort random noise.\n",
    "        x: [N, L, D], sequence\n",
    "        \"\"\"\n",
    "        N, L, D = x.shape  # batch, length, dim\n",
    "        len_keep = int(L * (1 - mask_ratio))\n",
    "\n",
    "        noise = torch.rand(N, L, device=x.device)  # noise in [0, 1]\n",
    "\n",
    "        # sort noise for each sample\n",
    "        ids_shuffle = torch.argsort(noise, dim=1)  # ascend: small is keep, large is remove\n",
    "        ids_restore = torch.argsort(ids_shuffle, dim=1)\n",
    "\n",
    "        # keep the first subset\n",
    "        ids_keep = ids_shuffle[:, :len_keep]\n",
    "        x_masked = torch.gather(x, dim=1, index=ids_keep.unsqueeze(-1).repeat(1, 1, D))\n",
    "\n",
    "        # generate the binary mask: 0 is keep, 1 is remove\n",
    "        mask = torch.ones([N, L], device=x.device)\n",
    "        mask[:, :len_keep] = 0\n",
    "        # unshuffle to get the binary mask\n",
    "        mask = torch.gather(mask, dim=1, index=ids_restore)\n",
    "\n",
    "        return x_masked, mask, ids_restore\n",
    "\n",
    "    def forward_encoder(self, x, mask_ratio):\n",
    "        # embed patches\n",
    "        x = self.patch_embed(x)\n",
    "\n",
    "        # add pos embed w/o cls token\n",
    "        x = x + self.pos_embed[:, 1:, :]\n",
    "\n",
    "        # masking: length -> length * mask_ratio\n",
    "        x, mask, ids_restore = self.random_masking(x, mask_ratio)\n",
    "\n",
    "        # append cls token\n",
    "        cls_token = self.cls_token + self.pos_embed[:, :1, :]\n",
    "        cls_tokens = cls_token.expand(x.shape[0], -1, -1)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "\n",
    "        # apply Transformer blocks\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x)\n",
    "        x = self.norm(x)\n",
    "\n",
    "        return x, mask, ids_restore\n",
    "\n",
    "    def forward_decoder(self, x, ids_restore):\n",
    "        # embed tokens\n",
    "        x = self.decoder_embed(x)\n",
    "\n",
    "        # append mask tokens to sequence\n",
    "        mask_tokens = self.mask_token.repeat(x.shape[0], ids_restore.shape[1] + 1 - x.shape[1], 1)\n",
    "        x_ = torch.cat([x[:, 1:, :], mask_tokens], dim=1)  # no cls token\n",
    "        x_ = torch.gather(x_, dim=1, index=ids_restore.unsqueeze(-1).repeat(1, 1, x.shape[2]))  # unshuffle\n",
    "        x = torch.cat([x[:, :1, :], x_], dim=1)  # append cls token\n",
    "\n",
    "        # add pos embed\n",
    "        x = x + self.decoder_pos_embed\n",
    "\n",
    "        # apply Transformer blocks\n",
    "        for blk in self.decoder_blocks:\n",
    "            x = blk(x)\n",
    "        x = self.decoder_norm(x)\n",
    "\n",
    "        # predictor projection\n",
    "        x = self.decoder_pred(x)\n",
    "\n",
    "        # remove cls token\n",
    "        x = x[:, 1:, :]\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward_loss(self, imgs, pred, mask):\n",
    "        \"\"\"\n",
    "        imgs: [N, 3, H, W]\n",
    "        pred: [N, L, p*p*3]\n",
    "        mask: [N, L], 0 is keep, 1 is remove,\n",
    "        \"\"\"\n",
    "        target = self.patchify(imgs)\n",
    "        if self.norm_pix_loss:\n",
    "            mean = target.mean(dim=-1, keepdim=True)\n",
    "            var = target.var(dim=-1, keepdim=True)\n",
    "            target = (target - mean) / (var + 1.e-6) ** .5\n",
    "\n",
    "        loss = (pred - target) ** 2\n",
    "        loss = loss.mean(dim=-1)  # [N, L], mean loss per patch\n",
    "\n",
    "        loss = (loss * mask).sum() / mask.sum()  # mean loss on removed patches\n",
    "        return loss\n",
    "\n",
    "    def forward(self, imgs, mask_ratio=0.75):\n",
    "        latent, mask, ids_restore = self.forward_encoder(imgs, mask_ratio)\n",
    "        pred = self.forward_decoder(latent, ids_restore)  # [N, L, p*p*3]\n",
    "        loss = self.forward_loss(imgs, pred, mask)\n",
    "        return loss, pred, mask\n",
    "\n",
    "\n",
    "\n",
    "def mae_vit_base_patch16_dec512d8b(**kwargs):\n",
    "    model = 掩码自编码器Vit(\n",
    "        patch_size=16, embed_dim=768, depth=12, num_heads=12,\n",
    "        decoder_embed_dim=512, decoder_depth=8, decoder_num_heads=16,\n",
    "        mlp_ratio=4, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
    "    return model\n",
    "\n",
    "测试 = mae_vit_base_patch16_dec512d8b()\n",
    "# 测试模型 = 立体图像块嵌入(304, 512)\n",
    "测试.to(torch.device('cuda:0'))\n",
    "# print('\\n')\n",
    "summary(测试, input_size=(3, 224, 224), batch_size=1, device='cuda') # [3, 256, 512]\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def drop_path(x, drop_prob: float = 0., training: bool = False):\n",
    "    \"\"\"\n",
    "    Drop paths (Stochastic Depth) per sample (when applied in main path of residual blocks).\n",
    "    This is the same as the DropConnect impl I created for EfficientNet, etc networks, however,\n",
    "    the original name is misleading as 'Drop Connect' is a different form of dropout in a separate paper...\n",
    "    See discussion: https://github.com/tensorflow/tpu/issues/494#issuecomment-532968956 ... I've opted for\n",
    "    changing the layer and argument names to 'drop path' rather than mix DropConnect as a layer name and use\n",
    "    'survival rate' as the argument.\n",
    "    \"\"\"\n",
    "    if drop_prob == 0. or not training:\n",
    "        return x\n",
    "    keep_prob = 1 - drop_prob\n",
    "    shape = (x.shape[0],) + (1,) * (x.ndim - 1)  # work with diff dim tensors, not just 2D ConvNets\n",
    "    random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)\n",
    "    random_tensor.floor_()  # binarize\n",
    "    output = x.div(keep_prob) * random_tensor\n",
    "    return output\n",
    "\n",
    "\n",
    "class DropPath(nn.Module):\n",
    "    \"\"\"\n",
    "    Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).\n",
    "    \"\"\"\n",
    "    def __init__(self, drop_prob=None):\n",
    "        super(DropPath, self).__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "\n",
    "    def forward(self, x):\n",
    "        return drop_path(x, self.drop_prob, self.training)\n",
    "\n",
    "\n",
    "class 自注意力(nn.Module):\n",
    "    \"\"\"\n",
    "    多头自注意力模块\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 嵌入向量维度,\n",
    "                 注意力头数量,\n",
    "                 qkv_偏差=False,\n",
    "                 自注意力丢弃率=0.,\n",
    "                 全连接层丢弃率=0.):\n",
    "        super(自注意力, self).__init__()\n",
    "        self.注意力头数量 = 注意力头数量\n",
    "        注意力头维度 = 嵌入向量维度 // 注意力头数量  # 每个注意力头的维度大小\n",
    "        self.qk_缩小因子 = 注意力头维度 ** -0.5\n",
    "        # 通过一个全连接层生成qkv三个向量，有助于并行化计算\n",
    "        # 小数据集中qkv直接false，但是原版用参数控制true\n",
    "        self.qkv = nn.Linear(嵌入向量维度, 嵌入向量维度 * 3, bias=qkv_偏差)\n",
    "        self.自注意力丢弃 = nn.Dropout(自注意力丢弃率) # 自注意力信息经过softmax后的Dropout\n",
    "\n",
    "        # 多头注意力的输出拼接后与Wo相乘得到最终的输出。Wo矩阵通过全连接层实现\n",
    "        self.线性投影 = nn.Linear(嵌入向量维度, 嵌入向量维度)\n",
    "        self.线性投影丢弃 = nn.Dropout(全连接层丢弃率)\n",
    "\n",
    "    def forward(self, x):\n",
    "        批量大小, 图像块数量, 嵌入向量维度 = x.shape\n",
    "        # [batch_size, num_patches + 1, embed_dim] 每一批图片数。图像块的数量加1是因为算上class token，我的方法按照纵向的深度计算\n",
    "        # qkv(): -> [批量大小, 图像块数量 + 1, 3 * 嵌入向量维度] 我不分类可能不加分类嵌入向量。\n",
    "        # 3对应qkv三个向量。后面两个维度的数据可以理解为将嵌入向量按照注意力头的数量平均划分之后分别送入不同的注意力头中计算\n",
    "        # reshape: -> [批量大小, 图像块数量 + 1, 3, 注意力头数量, 每个注意力头的嵌入向量维度]。\n",
    "        # permute: -> [3, 批量大小, 注意力头数量, 图像块数量 + 1, 每个注意力头的嵌入向量维度]\n",
    "        # TODO 报错\n",
    "        qkv = self.qkv(x).reshape(批量大小, 图像块数量, 3, self.注意力头数量, 嵌入向量维度 // self.注意力头数量).permute(2, 0, 3, 1, 4)\n",
    "\n",
    "        # 分别取出qkv向量。向量的形状[批量大小, 注意力头数量, 图像块数量 + 1, 每个注意力头的嵌入向量维度]\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "\n",
    "        # k最后两个维度调换位置，k.transpose：-> [批量大小, 注意力头数量, 每个注意力头的嵌入向量维度, 图像块数量 + 1]\n",
    "        # q和k矩阵相乘: -> [批量大小, 注意力头数量, 图像块数量 + 1, 图像块数量 + 1]\n",
    "        自注意力信息 = (q @ k.transpose(-2, -1)) * self.qk_缩小因子  # q和k进行多维矩阵乘法时，实际只有最后两个维度相乘\n",
    "        自注意力信息 = 自注意力信息.softmax(dim=-1)  # 沿着最后一个维度进行柔性最大值处理。\n",
    "        自注意力信息 = self.自注意力丢弃(自注意力信息)\n",
    "\n",
    "        # 自注意力信息和v矩阵相乘：-> [批量大小, 注意力头数量, 图像块数量 + 1, 每个注意力头的嵌入向量维度]\n",
    "        # transpose: -> [批量大小, 图像块数量 + 1, 注意力头数量, 每个注意力头的嵌入向量维度]\n",
    "        # reshape: -> [批量大小, 图像块数量 + 1, 嵌入向量维度]\n",
    "        自注意力信息 = (自注意力信息 @ v).transpose(1, 2).reshape(批量大小, 图像块数量, 嵌入向量维度)  # 和v矩阵相乘 加权求和\n",
    "        自注意力信息 = self.线性投影(自注意力信息)\n",
    "        自注意力信息 = self.线性投影丢弃(自注意力信息)\n",
    "        return 自注意力信息\n",
    "\n",
    "\n",
    "class 多层感知机(nn.Module):\n",
    "    \"\"\"\n",
    "    transformer编码块中的MLP模块\n",
    "    \"\"\"\n",
    "    def __init__(self, 输入大小, 隐藏层大小=None, 激活函数=nn.GELU, 丢弃率=0.):\n",
    "        \"\"\"\n",
    "\n",
    "        :param 输入大小: (int) 第一个全连接层的输入大小,默认和嵌入向量维度大小一致。\n",
    "        :param 隐藏层大小: (int)\n",
    "        :param 激活函数:\n",
    "        :param 丢弃率:\n",
    "        \"\"\"\n",
    "        super(多层感知机, self).__init__()\n",
    "        隐藏层大小 = 隐藏层大小 or 输入大小\n",
    "        self.全连接1 = nn.Linear(输入大小, 隐藏层大小)\n",
    "        self.激活函数 = 激活函数()\n",
    "        self.全连接2 = nn.Linear(隐藏层大小, 输入大小)\n",
    "        self.随机丢弃 = nn.Dropout(丢弃率)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.全连接1(x)\n",
    "        x = self.激活函数(x)\n",
    "        x = self.随机丢弃(x)\n",
    "        x = self.全连接2(x)\n",
    "        x = self.随机丢弃(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class 编码块(nn.Module):\n",
    "    \"\"\"\n",
    "    transformer编码器模块，将自注意力模块和MLP模块结合起来\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 特征维度, # 输入向量的维度\n",
    "                 注意力头数量, # 注意力头的数量\n",
    "                 多层感知机扩增率=4., # 第一个全连接层的节点是输入节点的四倍，对应768变为2304\n",
    "                 qkv_偏差=False, # 是否使用偏差\n",
    "                 自注意力丢弃率=0.,  # qkv矩阵计算输出之后经过softmax层后的dropout\n",
    "                 全连接层丢弃率=0., # 对应多头注意力模块中最后一个全连接层\n",
    "                 drop_path_ratio=0., # 对应的编码块中droppath\n",
    "                 激活函数=nn.GELU, # 默认激活函数\n",
    "                 标准化=nn.LayerNorm): # 默认归一化方式\n",
    "        super(编码块, self).__init__()\n",
    "        self.标准化1 = 标准化(特征维度) # 编码块中的第一个LN层\n",
    "        self.多头自注意力 = 自注意力(特征维度, 注意力头数量=注意力头数量, qkv_偏差=qkv_偏差, 自注意力丢弃率=自注意力丢弃率, 全连接层丢弃率=全连接层丢弃率)\n",
    "        # TODO 如果drop_path_ratio大于0，就采用droppath否则直接用恒等映射。作者认为droppath比dropout效果好\n",
    "        self.drop_path = DropPath(drop_path_ratio) if drop_path_ratio > 0. else nn.Identity()\n",
    "\n",
    "        self.标准化2 = 标准化(特征维度) # 对应MLP块中最前面的的LN层\n",
    "        隐藏层维度 = int(特征维度 * 多层感知机扩增率) # MLP模块中第一个全连接层对应的隐藏层节点数量，这里相当于维度增加。\n",
    "        self.多层感知机 = 多层感知机(输入大小=特征维度, 隐藏层大小=隐藏层维度, 激活函数=激活函数, 丢弃率=全连接层丢弃率)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.drop_path(self.多头自注意力(self.标准化1(x)))\n",
    "        x = x + self.drop_path(self.多层感知机(self.标准化2(x))) # 不能从这里直接修改多层感知机的输出维度，因为要和原输入x相加\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "class 平面图像块嵌入(nn.Module):\n",
    "    \"\"\"\n",
    "    如何将2维和3维整合在一起。3维输入的应该是\n",
    "    现在认为输入的图像都是已经划分成图像块之后的结果。有监督训练的时候在另一个类里对有标签的图像划分。\n",
    "    \"\"\"\n",
    "    def __init__(self, 图像形状, 图像块嵌入向量的维度=256, 非卷积分块=False):\n",
    "        r\"\"\"\n",
    "        加if分支处理3维和2维的区别\n",
    "        输入通道数暂时没用到，有监督训练时如何对输入的图像划分\n",
    "        3维输入数据形状BCDHW 1*2*256*16*16，在256中随机选取部分用来训练，输出1*256的向量，然后整形为1*16*16。\n",
    "\n",
    "        :param 图像形状:\n",
    "        :param 图像块嵌入向量的维度:\n",
    "        \"\"\"\n",
    "        # todo 现在是3维为例\n",
    "        super(平面图像块嵌入, self).__init__()\n",
    "        self.图像形状 = 图像形状\n",
    "        self.图像块的大小 = 16\n",
    "        self.图像块数量 = 256 # TODO 注意修改\n",
    "        self.标准化 = nn.LayerNorm(图像块嵌入向量的维度)\n",
    "        self.非卷积分块 = 非卷积分块\n",
    "        if self.非卷积分块:\n",
    "            self.图像分块 = nn.Unfold(kernel_size=16, stride=16)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        # 分块函数 F.unfold\n",
    "        # 通过立体图像输入应该是[256*16*16]，但我真实的图片应该是[批量，宽度，高度，通道数]\n",
    "        # TODO 现在只考虑从立体数据融合来的。输入256*16*16，展平后256*256。\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        # 再展平后输出1*256*512，形状[B, D, HWC]\n",
    "        x = torch.flatten(x, 2)\n",
    "        x = self.标准化(x)\n",
    "        \"\"\"\n",
    "        if self.非卷积分块:\n",
    "            x = self.图像分块(x)\n",
    "        return x\n",
    "# 测试 = 平面图像块嵌入(304, 非卷积分块=False)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "class 直接平面图像块嵌入(nn.Module):\n",
    "    \"\"\"\n",
    "    如何将2维和3维整合在一起。3维输入的应该是\n",
    "    现在认为输入的图像都是已经划分成图像块之后的结果。有监督训练的时候在另一个类里对有标签的图像划分。\n",
    "    \"\"\"\n",
    "    def __init__(self, 图像形状, 图像块嵌入向量的维度=256):\n",
    "        \"\"\"\n",
    "        加if分支处理3维和2维的区别\n",
    "        输入通道数暂时没用到，有监督训练时如何对输入的图像划分\n",
    "        3维输入数据形状BCDHW 1*2*256*16*16，在256中随机选取部分用来训练，输出1*256的向量，然后整形为1*16*16。\n",
    "\n",
    "        :param 图像形状:\n",
    "        :param 图像块嵌入向量的维度:\n",
    "        \"\"\"\n",
    "        # todo 现在是3维为例\n",
    "        super().__init__()\n",
    "        self.图像形状 = 图像形状\n",
    "        self.图像块的大小 = 16\n",
    "        self.图像块数量 = 361 # TODO 注意修改\n",
    "        self.标准化 = nn.LayerNorm(图像块嵌入向量的维度)\n",
    "        # self.图像分块 = nn.Unfold(kernel_size=16, stride=16)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 通过立体图像输入应该是[1*361*16*16]，但我真实的图片应该是[批量，宽度，高度，通道数]\n",
    "        # TODO 现在只考虑从立体数据融合来的。输入1*361*16*16，展平后1*361*256。\n",
    "        x = torch.flatten(x, start_dim=2)\n",
    "        x = self.标准化(x) # 输出形状[1*361*256]\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 361, 256])\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "         LayerNorm-1              [1, 361, 256]             512\n",
      "         直接平面图像块嵌入-2              [1, 361, 256]               0\n",
      "         LayerNorm-3               [1, 91, 256]             512\n",
      "            Linear-4               [1, 91, 768]         197,376\n",
      "           Dropout-5             [1, 2, 91, 91]               0\n",
      "            Linear-6               [1, 91, 256]          65,792\n",
      "           Dropout-7               [1, 91, 256]               0\n",
      "              自注意力-8               [1, 91, 256]               0\n",
      "          Identity-9               [1, 91, 256]               0\n",
      "        LayerNorm-10               [1, 91, 256]             512\n",
      "           Linear-11              [1, 91, 1024]         263,168\n",
      "             GELU-12              [1, 91, 1024]               0\n",
      "          Dropout-13              [1, 91, 1024]               0\n",
      "           Linear-14               [1, 91, 256]         262,400\n",
      "          Dropout-15               [1, 91, 256]               0\n",
      "            多层感知机-16               [1, 91, 256]               0\n",
      "         Identity-17               [1, 91, 256]               0\n",
      "              编码块-18               [1, 91, 256]               0\n",
      "        LayerNorm-19               [1, 91, 256]             512\n",
      "           Linear-20               [1, 91, 256]          65,792\n",
      "        LayerNorm-21              [1, 362, 256]             512\n",
      "           Linear-22              [1, 362, 768]         197,376\n",
      "          Dropout-23          [1, 16, 362, 362]               0\n",
      "           Linear-24              [1, 362, 256]          65,792\n",
      "          Dropout-25              [1, 362, 256]               0\n",
      "             自注意力-26              [1, 362, 256]               0\n",
      "         Identity-27              [1, 362, 256]               0\n",
      "        LayerNorm-28              [1, 362, 256]             512\n",
      "           Linear-29             [1, 362, 1024]         263,168\n",
      "             GELU-30             [1, 362, 1024]               0\n",
      "          Dropout-31             [1, 362, 1024]               0\n",
      "           Linear-32              [1, 362, 256]         262,400\n",
      "          Dropout-33              [1, 362, 256]               0\n",
      "            多层感知机-34              [1, 362, 256]               0\n",
      "         Identity-35              [1, 362, 256]               0\n",
      "              编码块-36              [1, 362, 256]               0\n",
      "        LayerNorm-37              [1, 362, 256]             512\n",
      "           Linear-38              [1, 362, 768]         197,376\n",
      "          Dropout-39          [1, 16, 362, 362]               0\n",
      "           Linear-40              [1, 362, 256]          65,792\n",
      "          Dropout-41              [1, 362, 256]               0\n",
      "             自注意力-42              [1, 362, 256]               0\n",
      "         Identity-43              [1, 362, 256]               0\n",
      "        LayerNorm-44              [1, 362, 256]             512\n",
      "           Linear-45             [1, 362, 1024]         263,168\n",
      "             GELU-46             [1, 362, 1024]               0\n",
      "          Dropout-47             [1, 362, 1024]               0\n",
      "           Linear-48              [1, 362, 256]         262,400\n",
      "          Dropout-49              [1, 362, 256]               0\n",
      "            多层感知机-50              [1, 362, 256]               0\n",
      "         Identity-51              [1, 362, 256]               0\n",
      "              编码块-52              [1, 362, 256]               0\n",
      "        LayerNorm-53              [1, 362, 256]             512\n",
      "           Linear-54              [1, 362, 768]         197,376\n",
      "          Dropout-55          [1, 16, 362, 362]               0\n",
      "           Linear-56              [1, 362, 256]          65,792\n",
      "          Dropout-57              [1, 362, 256]               0\n",
      "             自注意力-58              [1, 362, 256]               0\n",
      "         Identity-59              [1, 362, 256]               0\n",
      "        LayerNorm-60              [1, 362, 256]             512\n",
      "           Linear-61             [1, 362, 1024]         263,168\n",
      "             GELU-62             [1, 362, 1024]               0\n",
      "          Dropout-63             [1, 362, 1024]               0\n",
      "           Linear-64              [1, 362, 256]         262,400\n",
      "          Dropout-65              [1, 362, 256]               0\n",
      "            多层感知机-66              [1, 362, 256]               0\n",
      "         Identity-67              [1, 362, 256]               0\n",
      "              编码块-68              [1, 362, 256]               0\n",
      "        LayerNorm-69              [1, 362, 256]             512\n",
      "           Linear-70              [1, 362, 768]         197,376\n",
      "          Dropout-71          [1, 16, 362, 362]               0\n",
      "           Linear-72              [1, 362, 256]          65,792\n",
      "          Dropout-73              [1, 362, 256]               0\n",
      "             自注意力-74              [1, 362, 256]               0\n",
      "         Identity-75              [1, 362, 256]               0\n",
      "        LayerNorm-76              [1, 362, 256]             512\n",
      "           Linear-77             [1, 362, 1024]         263,168\n",
      "             GELU-78             [1, 362, 1024]               0\n",
      "          Dropout-79             [1, 362, 1024]               0\n",
      "           Linear-80              [1, 362, 256]         262,400\n",
      "          Dropout-81              [1, 362, 256]               0\n",
      "            多层感知机-82              [1, 362, 256]               0\n",
      "         Identity-83              [1, 362, 256]               0\n",
      "              编码块-84              [1, 362, 256]               0\n",
      "        LayerNorm-85              [1, 362, 256]             512\n",
      "           Linear-86              [1, 362, 768]         197,376\n",
      "          Dropout-87          [1, 16, 362, 362]               0\n",
      "           Linear-88              [1, 362, 256]          65,792\n",
      "          Dropout-89              [1, 362, 256]               0\n",
      "             自注意力-90              [1, 362, 256]               0\n",
      "         Identity-91              [1, 362, 256]               0\n",
      "        LayerNorm-92              [1, 362, 256]             512\n",
      "           Linear-93             [1, 362, 1024]         263,168\n",
      "             GELU-94             [1, 362, 1024]               0\n",
      "          Dropout-95             [1, 362, 1024]               0\n",
      "           Linear-96              [1, 362, 256]         262,400\n",
      "          Dropout-97              [1, 362, 256]               0\n",
      "            多层感知机-98              [1, 362, 256]               0\n",
      "         Identity-99              [1, 362, 256]               0\n",
      "             编码块-100              [1, 362, 256]               0\n",
      "       LayerNorm-101              [1, 362, 256]             512\n",
      "          Linear-102              [1, 362, 768]         197,376\n",
      "         Dropout-103          [1, 16, 362, 362]               0\n",
      "          Linear-104              [1, 362, 256]          65,792\n",
      "         Dropout-105              [1, 362, 256]               0\n",
      "            自注意力-106              [1, 362, 256]               0\n",
      "        Identity-107              [1, 362, 256]               0\n",
      "       LayerNorm-108              [1, 362, 256]             512\n",
      "          Linear-109             [1, 362, 1024]         263,168\n",
      "            GELU-110             [1, 362, 1024]               0\n",
      "         Dropout-111             [1, 362, 1024]               0\n",
      "          Linear-112              [1, 362, 256]         262,400\n",
      "         Dropout-113              [1, 362, 256]               0\n",
      "           多层感知机-114              [1, 362, 256]               0\n",
      "        Identity-115              [1, 362, 256]               0\n",
      "             编码块-116              [1, 362, 256]               0\n",
      "       LayerNorm-117              [1, 362, 256]             512\n",
      "          Linear-118              [1, 362, 768]         197,376\n",
      "         Dropout-119          [1, 16, 362, 362]               0\n",
      "          Linear-120              [1, 362, 256]          65,792\n",
      "         Dropout-121              [1, 362, 256]               0\n",
      "            自注意力-122              [1, 362, 256]               0\n",
      "        Identity-123              [1, 362, 256]               0\n",
      "       LayerNorm-124              [1, 362, 256]             512\n",
      "          Linear-125             [1, 362, 1024]         263,168\n",
      "            GELU-126             [1, 362, 1024]               0\n",
      "         Dropout-127             [1, 362, 1024]               0\n",
      "          Linear-128              [1, 362, 256]         262,400\n",
      "         Dropout-129              [1, 362, 256]               0\n",
      "           多层感知机-130              [1, 362, 256]               0\n",
      "        Identity-131              [1, 362, 256]               0\n",
      "             编码块-132              [1, 362, 256]               0\n",
      "       LayerNorm-133              [1, 362, 256]             512\n",
      "          Linear-134              [1, 362, 768]         197,376\n",
      "         Dropout-135          [1, 16, 362, 362]               0\n",
      "          Linear-136              [1, 362, 256]          65,792\n",
      "         Dropout-137              [1, 362, 256]               0\n",
      "            自注意力-138              [1, 362, 256]               0\n",
      "        Identity-139              [1, 362, 256]               0\n",
      "       LayerNorm-140              [1, 362, 256]             512\n",
      "          Linear-141             [1, 362, 1024]         263,168\n",
      "            GELU-142             [1, 362, 1024]               0\n",
      "         Dropout-143             [1, 362, 1024]               0\n",
      "          Linear-144              [1, 362, 256]         262,400\n",
      "         Dropout-145              [1, 362, 256]               0\n",
      "           多层感知机-146              [1, 362, 256]               0\n",
      "        Identity-147              [1, 362, 256]               0\n",
      "             编码块-148              [1, 362, 256]               0\n",
      "       LayerNorm-149              [1, 362, 256]             512\n",
      "          Linear-150              [1, 362, 256]          65,792\n",
      "================================================================\n",
      "Total params: 7,240,960\n",
      "Trainable params: 7,240,960\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.35\n",
      "Forward/backward pass size (MB): 282.96\n",
      "Params size (MB): 27.62\n",
      "Estimated Total Size (MB): 310.94\n",
      "----------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\XK\\AppData\\Local\\Temp\\ipykernel_320\\1758348278.py:19: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  omega = np.arange(embed_dim // 2, dtype=np.float)\n"
     ]
    }
   ],
   "source": [
    "class 直接自监督重建OCTA图像(nn.Module):\n",
    "    \"\"\"\n",
    "    现在考虑传进来的数据是已经划分块之后的结果，图像是单通道\n",
    "    \"\"\"\n",
    "    def __init__(self, 图像形状=224, 图像块大小=16, 嵌入向量维度=256, 深度=1, 注意力头数量=2, 解码器嵌入向量维度=256, 解码器深度=8,\n",
    "                 解码器注意力头数量=16, 多层感知机扩增率=4., 标准化=nn.LayerNorm, norm_pix_loss=False):\n",
    "        super().__init__()\n",
    "\n",
    "        # --------------------------------------------------------------------------\n",
    "        # MAE编码器实现\n",
    "        self.图像块嵌入向量 = 直接平面图像块嵌入(图像形状, 嵌入向量维度)\n",
    "        图像块数量 = self.图像块嵌入向量.图像块数量\n",
    "        # 第一个轴的1表示批量维度\n",
    "        self.分类嵌入 = nn.Parameter(torch.zeros(1, 1, 嵌入向量维度))\n",
    "        self.位置嵌入向量 = nn.Parameter(torch.zeros(1, 图像块数量 + 1, 嵌入向量维度),\n",
    "                                      requires_grad=False)  # fixed sin-cos embedding\n",
    "        # 作者没给transformer编码块传入dropout的丢弃率\n",
    "        self.编码块堆叠 = nn.Sequential(*[\n",
    "            编码块(特征维度=嵌入向量维度, 注意力头数量=注意力头数量, 多层感知机扩增率=多层感知机扩增率, qkv_偏差=True, 标准化=标准化)\n",
    "            for i in range(深度)])\n",
    "        self.编码器标准化 = 标准化(嵌入向量维度)\n",
    "        # --------------------------------------------------------------------------\n",
    "\n",
    "        # --------------------------------------------------------------------------\n",
    "        # MAE解码器实现\n",
    "        self.解码嵌入 = nn.Linear(嵌入向量维度, 解码器嵌入向量维度, bias=True)\n",
    "\n",
    "        # 替换被遮掩的图像块\n",
    "        self.mask_token = nn.Parameter(torch.zeros(1, 1, 解码器嵌入向量维度))\n",
    "\n",
    "        # 加1是因为解码也需要cls_token\n",
    "        self.解码器位置嵌入向量 = nn.Parameter(torch.zeros(1, 图像块数量 + 1, 解码器嵌入向量维度),\n",
    "                                              requires_grad=False)  # fixed sin-cos embedding\n",
    "\n",
    "        self.解码块堆叠 = nn.Sequential(*[\n",
    "            编码块(特征维度=解码器嵌入向量维度, 注意力头数量=解码器注意力头数量, 多层感知机扩增率=多层感知机扩增率, qkv_偏差=True, 标准化=标准化)\n",
    "            for i in range(解码器深度)])\n",
    "\n",
    "        self.解码器标准化 = 标准化(解码器嵌入向量维度)\n",
    "        # 这个就是输出时的最后一层了\n",
    "        self.解码器预测值 = nn.Linear(解码器嵌入向量维度, 图像块大小 ** 2, bias=True)  # decoder to patch\n",
    "        # --------------------------------------------------------------------------\n",
    "\n",
    "        self.norm_pix_loss = norm_pix_loss\n",
    "\n",
    "        # 初始化 (and freeze) pos_embed by sin-cos embedding\n",
    "        # 编码器的位置嵌入\n",
    "        位置嵌入向量 = get_2d_sincos_pos_embed(self.位置嵌入向量.shape[-1], int(self.图像块嵌入向量.图像块数量 ** .5),\n",
    "                                            cls_token=True)\n",
    "        self.位置嵌入向量.data.copy_(torch.from_numpy(位置嵌入向量).float().unsqueeze(0))\n",
    "\n",
    "        解码器位置嵌入向量 = get_2d_sincos_pos_embed(self.解码器位置嵌入向量.shape[-1],\n",
    "                                                    int(self.图像块嵌入向量.图像块数量 ** .5), cls_token=True)\n",
    "        self.解码器位置嵌入向量.data.copy_(torch.from_numpy(解码器位置嵌入向量).float().unsqueeze(0))\n",
    "\n",
    "        # TODO 如果在生成图像块嵌入向量的时候使用卷积操作才用到这个初始化操作\n",
    "        # initialize patch_embed like nn.Linear (instead of nn.Conv2d)\n",
    "        # 权重 = self.图像块嵌入向量.proj.weight.data\n",
    "        # torch.nn.init.xavier_uniform_(w.view([w.shape[0], -1]))\n",
    "\n",
    "        # timm's trunc_normal_(std=.02) is effectively normal_(std=0.02) as cutoff is too big (2.)\n",
    "        # torch.nn.init.normal_(self.cls_token, std=.02)\n",
    "        torch.nn.init.normal_(self.mask_token, std=.02)\n",
    "\n",
    "        # initialize nn.Linear and nn.LayerNorm\n",
    "        self.apply(self.初始化权重)\n",
    "\n",
    "    def 初始化权重(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            torch.nn.init.xavier_uniform_(m.weight)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "    # 将图片划分成块，计算损失函数的时候用到，我现在已经就是块了所以不需要\n",
    "    def patchify(self, imgs):\n",
    "        \"\"\"\n",
    "        vit中切分patch调整数据维度的操作\n",
    "        imgs: (批量, 3, H, W)\n",
    "        x: (批量, L, patch_size**2 *3)\n",
    "        \"\"\"\n",
    "        块大小 = self.图像块嵌入向量.图像块的大小 # 每个patch的长宽\n",
    "        assert imgs.shape[2] == imgs.shape[3] and imgs.shape[2] % 块大小 == 0\n",
    "\n",
    "        # 原图像的高度和宽度对块的大小做整数除法\n",
    "        高度数量 = 宽度数量 = imgs.shape[2] // 块大小  # TODO 计算patch的数量？怎么计算的\n",
    "        x = imgs.reshape(shape=(imgs.shape[0], 3, 高度数量, 块大小, 宽度数量, 块大小))\n",
    "        # batchsize，通道数，patch每一边的数量，每个patch的宽高，patch每一边的数量，每个patch的长宽\n",
    "        x = torch.einsum('nchpwq->nhwpqc', x)\n",
    "        x = x.reshape(shape=(imgs.shape[0], 高度数量 * 宽度数量, 块大小 ** 2 * 3))\n",
    "        # 其实直接转换过来就行， （batchsize，patch总数base=196，path平方x通道数也就是每个patch块内的数据）\n",
    "        return x\n",
    "\n",
    "    # 将图像块还原成完整的图像\n",
    "    def unpatchify(self, x):\n",
    "        \"\"\"\n",
    "        x: (批量, L, patch_size**2 *3)\n",
    "        imgs: (批量, 3, H, W)\n",
    "        \"\"\"\n",
    "        p = self.patch_embed.patch_size[0]\n",
    "        h = w = int(x.shape[1] ** .5)\n",
    "        assert h * w == x.shape[1]\n",
    "\n",
    "        x = x.reshape(shape=(x.shape[0], h, w, p, p, 3))\n",
    "        x = torch.einsum('nhwpqc->nchpwq', x)\n",
    "        imgs = x.reshape(shape=(x.shape[0], 3, h * p, h * p))\n",
    "        # （batchsize，patch总数base=196，path平方x通道数也就是每个patch块内的数据）->（图像宽高，通道数，hxp=patch[0]xp的宽高）\n",
    "        return imgs\n",
    "\n",
    "    # TODO 随机掩码函数，很重要\n",
    "    def 随机掩码(self, x, 掩码率):\n",
    "        \"\"\"\n",
    "        Perform per-sample random masking by per-sample shuffling.\n",
    "        Per-sample shuffling is done by argsort random noise.\n",
    "        x: [批量, L, 嵌入向量维度], sequence\n",
    "        \"\"\"\n",
    "\n",
    "        批量, 块数量, 嵌入向量维度 = x.shape  # 获取输入数据的形状，批量, 块数量, 维度\n",
    "        保留块数量 = int(块数量 * (1 - 掩码率))\n",
    "        noise = torch.rand(批量, 块数量, device=x.device)  # 二维随机噪声矩阵，数值在[0, 1]\n",
    "\n",
    "        # sort noise for each sample。argsort()返回的是元素对应的索引\n",
    "        # ids_shuffle是用来选择哪些元素做掩码\n",
    "        ids_shuffle = torch.argsort(noise, dim=1)  # 升序排列ascend: small is keep, large is remove\n",
    "        # ids_restore是用于在编码之后对图像块的顺序进行还原并输入解码器？\n",
    "        ids_restore = torch.argsort(ids_shuffle, dim=1) # 对上一步得到的索引排序\n",
    "\n",
    "        # keep the first subset\n",
    "        ids_keep = ids_shuffle[:, :保留块数量]\n",
    "        # 没有被掩码的图像块序列？\n",
    "        x_masked = torch.gather(x, dim=1, index=ids_keep.unsqueeze(-1).repeat(1, 1, 嵌入向量维度))\n",
    "\n",
    "        # generate the binary mask: 0 is keep, 1 is remove\n",
    "        #\n",
    "        mask = torch.ones([批量, 块数量], device=x.device)\n",
    "        mask[:, :保留块数量] = 0\n",
    "        # unshuffle to get the binary mask\n",
    "        mask = torch.gather(mask, dim=1, index=ids_restore) #\n",
    "\n",
    "        return x_masked, mask, ids_restore\n",
    "\n",
    "    def forward_encoder(self, x, 掩码率):\n",
    "        # TODO 现在测试直接用已经分块的图像，不是真实图像再分块\n",
    "        x = self.图像块嵌入向量(x) # 图像块转化为嵌入向量 [1*361*256]\n",
    "\n",
    "        # add pos embed w/o cls token\n",
    "        x = x + self.位置嵌入向量[:, 1:, :] # 从1开始是因为0对应cls_token\n",
    "\n",
    "        # masking: length -> length * 掩码率。\n",
    "        # TODO 关键操作\n",
    "        x, mask, ids_restore = self.随机掩码(x, 掩码率)\n",
    "\n",
    "        # 添加分类嵌入向量\n",
    "        分类嵌入 = self.分类嵌入 + self.位置嵌入向量[:, :1, :]\n",
    "        分类嵌入 = 分类嵌入.expand(x.shape[0], -1, -1)\n",
    "        x = torch.cat((分类嵌入, x), dim=1)\n",
    "\n",
    "        x = self.编码块堆叠(x)\n",
    "        x = self.编码器标准化(x)\n",
    "\n",
    "        return x, mask, ids_restore\n",
    "\n",
    "    def forward_decoder(self, x, ids_restore):\n",
    "        # 编码器的输出转化为解码器的输入嵌入向量\n",
    "        x = self.解码嵌入(x)\n",
    "\n",
    "        # 加上被掩码的图像\n",
    "        mask_tokens = self.mask_token.repeat(x.shape[0], ids_restore.shape[1] + 1 - x.shape[1], 1)\n",
    "        x_ = torch.cat([x[:, 1:, :], mask_tokens], dim=1)  # no cls token\n",
    "        x_ = torch.gather(x_, dim=1, index=ids_restore.unsqueeze(-1).repeat(1, 1, x.shape[2]))  # unshuffle\n",
    "        x = torch.cat([x[:, :1, :], x_], dim=1)  # append cls token\n",
    "\n",
    "        # 添加解码器位置嵌入向量\n",
    "        x = x + self.解码器位置嵌入向量\n",
    "        x = self.解码块堆叠(x)\n",
    "        x = self.解码器标准化(x)\n",
    "        x = self.解码器预测值(x)\n",
    "\n",
    "        x = x[:, 1:, :] # 取出没有分类嵌入向量的部分\n",
    "        print(x.shape)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward_loss(self, imgs, pred, mask):\n",
    "        \"\"\"\n",
    "        imgs: [批量, 3, H, W]\n",
    "        pred: [批量, L, p*p*3]\n",
    "        mask: [批量, L], 0 is keep, 1 is remove,\n",
    "        \"\"\"\n",
    "\n",
    "        target = self.patchify(imgs)\n",
    "        if self.norm_pix_loss:\n",
    "            mean = target.mean(dim=-1, keepdim=True)\n",
    "            var = target.var(dim=-1, keepdim=True)\n",
    "            target = (target - mean) / (var + 1.e-6) ** .5\n",
    "\n",
    "        loss = (pred - target) ** 2\n",
    "        loss = loss.mean(dim=-1)  # [批量, L], mean loss per patch\n",
    "\n",
    "        loss = (loss * mask).sum() / mask.sum()  # mean loss on removed patches\n",
    "        return loss\n",
    "\n",
    "    def forward(self, imgs, 掩码率=0.75):\n",
    "        # img是原始且未分割的图片\n",
    "        latent, mask, ids_restore = self.forward_encoder(imgs, 掩码率)\n",
    "        pred = self.forward_decoder(latent, ids_restore)  # [批量, L, p*p*3]\n",
    "        # loss = self.forward_loss(imgs, pred, mask)\n",
    "        return pred, mask\n",
    "        # return loss, pred, mask\n",
    "测试 = 直接自监督重建OCTA图像()\n",
    "# 测试模型 = 立体图像块嵌入(304, 512)\n",
    "测试.to(torch.device('cuda:0'))\n",
    "# print('\\n')\n",
    "summary(测试, input_size=(361, 16, 16), batch_size=1, device='cuda') # [3, 256, 512]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class 直接自监督重建OCTA图像(nn.Module):\n",
    "    \"\"\"\n",
    "    现在考虑传进来的数据是已经划分块之后的结果，图像是单通道\n",
    "    \"\"\"\n",
    "    def __init__(self, 图像形状=224, 图像块大小=16, 嵌入向量维度=256, 深度=1, 注意力头数量=2, 解码器嵌入向量维度=256, 解码器深度=8,\n",
    "                 解码器注意力头数量=16, 多层感知机扩增率=4., 标准化=nn.LayerNorm, norm_pix_loss=False):\n",
    "        super().__init__()\n",
    "\n",
    "        # --------------------------------------------------------------------------\n",
    "        # MAE编码器实现\n",
    "        self.图像块嵌入向量 = 直接平面图像块嵌入(图像形状, 嵌入向量维度)\n",
    "        图像块数量 = self.图像块嵌入向量.图像块数量\n",
    "        # 第一个轴的1表示批量维度\n",
    "        self.分类嵌入 = nn.Parameter(torch.zeros(1, 1, 嵌入向量维度))\n",
    "        self.位置嵌入向量 = nn.Parameter(torch.zeros(1, 图像块数量 + 1, 嵌入向量维度),\n",
    "                                      requires_grad=False)  # fixed sin-cos embedding\n",
    "        # 作者没给transformer编码块传入dropout的丢弃率\n",
    "        self.编码块堆叠 = nn.Sequential(*[\n",
    "            编码块(特征维度=嵌入向量维度, 注意力头数量=注意力头数量, 多层感知机扩增率=多层感知机扩增率, qkv_偏差=True, 标准化=标准化)\n",
    "            for i in range(深度)])\n",
    "        self.编码器标准化 = 标准化(嵌入向量维度)\n",
    "        # --------------------------------------------------------------------------\n",
    "\n",
    "        # --------------------------------------------------------------------------\n",
    "        # MAE解码器实现\n",
    "        self.解码嵌入 = nn.Linear(嵌入向量维度, 解码器嵌入向量维度, bias=True)\n",
    "\n",
    "        # 替换被遮掩的图像块\n",
    "        self.mask_token = nn.Parameter(torch.zeros(1, 1, 解码器嵌入向量维度))\n",
    "\n",
    "        # 加1是因为解码也需要cls_token\n",
    "        self.解码器位置嵌入向量 = nn.Parameter(torch.zeros(1, 图像块数量 + 1, 解码器嵌入向量维度),\n",
    "                                              requires_grad=False)  # fixed sin-cos embedding\n",
    "\n",
    "        self.解码块堆叠 = nn.Sequential(*[\n",
    "            编码块(特征维度=解码器嵌入向量维度, 注意力头数量=解码器注意力头数量, 多层感知机扩增率=多层感知机扩增率, qkv_偏差=True, 标准化=标准化)\n",
    "            for i in range(解码器深度)])\n",
    "\n",
    "        self.解码器标准化 = 标准化(解码器嵌入向量维度)\n",
    "        # 这个就是输出时的最后一层了\n",
    "        self.解码器预测值 = nn.Linear(解码器嵌入向量维度, 图像块大小 ** 2, bias=True)  # decoder to patch\n",
    "        # --------------------------------------------------------------------------\n",
    "\n",
    "        self.norm_pix_loss = norm_pix_loss\n",
    "\n",
    "        # 初始化 (and freeze) pos_embed by sin-cos embedding\n",
    "        # 编码器的位置嵌入\n",
    "        位置嵌入向量 = get_2d_sincos_pos_embed(self.位置嵌入向量.shape[-1], int(self.图像块嵌入向量.图像块数量 ** .5),\n",
    "                                            cls_token=True)\n",
    "        self.位置嵌入向量.data.copy_(torch.from_numpy(位置嵌入向量).float().unsqueeze(0))\n",
    "\n",
    "        解码器位置嵌入向量 = get_2d_sincos_pos_embed(self.解码器位置嵌入向量.shape[-1],\n",
    "                                                    int(self.图像块嵌入向量.图像块数量 ** .5), cls_token=True)\n",
    "        self.解码器位置嵌入向量.data.copy_(torch.from_numpy(解码器位置嵌入向量).float().unsqueeze(0))\n",
    "\n",
    "        # TODO 如果在生成图像块嵌入向量的时候使用卷积操作才用到这个初始化操作\n",
    "        # initialize patch_embed like nn.Linear (instead of nn.Conv2d)\n",
    "        # 权重 = self.图像块嵌入向量.proj.weight.data\n",
    "        # torch.nn.init.xavier_uniform_(w.view([w.shape[0], -1]))\n",
    "\n",
    "        # timm's trunc_normal_(std=.02) is effectively normal_(std=0.02) as cutoff is too big (2.)\n",
    "        # torch.nn.init.normal_(self.cls_token, std=.02)\n",
    "        torch.nn.init.normal_(self.mask_token, std=.02)\n",
    "\n",
    "        # initialize nn.Linear and nn.LayerNorm\n",
    "        self.apply(self.初始化权重)\n",
    "\n",
    "    def 初始化权重(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            torch.nn.init.xavier_uniform_(m.weight)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "    # 将图片划分成块，计算损失函数的时候用到，我现在已经就是块了所以不需要\n",
    "    def patchify(self, imgs):\n",
    "        \"\"\"\n",
    "        vit中切分patch调整数据维度的操作\n",
    "        imgs: (批量, 3, H, W)\n",
    "        x: (批量, L, patch_size**2 *3)\n",
    "        \"\"\"\n",
    "        块大小 = self.图像块嵌入向量.图像块的大小 # 每个patch的长宽\n",
    "        assert imgs.shape[2] == imgs.shape[3] and imgs.shape[2] % 块大小 == 0\n",
    "\n",
    "        # 原图像的高度和宽度对块的大小做整数除法\n",
    "        高度数量 = 宽度数量 = imgs.shape[2] // 块大小  # TODO 计算patch的数量？怎么计算的\n",
    "        x = imgs.reshape(shape=(imgs.shape[0], 3, 高度数量, 块大小, 宽度数量, 块大小))\n",
    "        # batchsize，通道数，patch每一边的数量，每个patch的宽高，patch每一边的数量，每个patch的长宽\n",
    "        x = torch.einsum('nchpwq->nhwpqc', x)\n",
    "        x = x.reshape(shape=(imgs.shape[0], 高度数量 * 宽度数量, 块大小 ** 2 * 3))\n",
    "        # 其实直接转换过来就行， （batchsize，patch总数base=196，path平方x通道数也就是每个patch块内的数据）\n",
    "        return x\n",
    "\n",
    "    # 将图像块还原成完整的图像\n",
    "    def unpatchify(self, x):\n",
    "        \"\"\"\n",
    "        x: (批量, L, patch_size**2 *3)\n",
    "        imgs: (批量, 3, H, W)\n",
    "        \"\"\"\n",
    "        p = self.patch_embed.patch_size[0]\n",
    "        h = w = int(x.shape[1] ** .5)\n",
    "        assert h * w == x.shape[1]\n",
    "\n",
    "        x = x.reshape(shape=(x.shape[0], h, w, p, p, 3))\n",
    "        x = torch.einsum('nhwpqc->nchpwq', x)\n",
    "        imgs = x.reshape(shape=(x.shape[0], 3, h * p, h * p))\n",
    "        # （batchsize，patch总数base=196，path平方x通道数也就是每个patch块内的数据）->（图像宽高，通道数，hxp=patch[0]xp的宽高）\n",
    "        return imgs\n",
    "\n",
    "    # TODO 随机掩码函数，很重要\n",
    "    def 随机掩码(self, x, 掩码率):\n",
    "        \"\"\"\n",
    "        Perform per-sample random masking by per-sample shuffling.\n",
    "        Per-sample shuffling is done by argsort random noise.\n",
    "        x: [批量, L, 嵌入向量维度], sequence\n",
    "        \"\"\"\n",
    "\n",
    "        批量, 块数量, 嵌入向量维度 = x.shape  # 获取输入数据的形状，批量, 块数量, 维度\n",
    "        保留块数量 = int(块数量 * (1 - 掩码率))\n",
    "        noise = torch.rand(批量, 块数量, device=x.device)  # 二维随机噪声矩阵，数值在[0, 1]\n",
    "\n",
    "        # sort noise for each sample。argsort()返回的是元素对应的索引\n",
    "        # ids_shuffle是用来选择哪些元素做掩码\n",
    "        ids_shuffle = torch.argsort(noise, dim=1)  # 升序排列ascend: small is keep, large is remove\n",
    "        # ids_restore是用于在编码之后对图像块的顺序进行还原并输入解码器？\n",
    "        ids_restore = torch.argsort(ids_shuffle, dim=1) # 对上一步得到的索引排序\n",
    "\n",
    "        # keep the first subset\n",
    "        ids_keep = ids_shuffle[:, :保留块数量]\n",
    "        # 没有被掩码的图像块序列？\n",
    "        x_masked = torch.gather(x, dim=1, index=ids_keep.unsqueeze(-1).repeat(1, 1, 嵌入向量维度))\n",
    "\n",
    "        # generate the binary mask: 0 is keep, 1 is remove\n",
    "        #\n",
    "        mask = torch.ones([批量, 块数量], device=x.device)\n",
    "        mask[:, :保留块数量] = 0\n",
    "        # unshuffle to get the binary mask\n",
    "        mask = torch.gather(mask, dim=1, index=ids_restore) #\n",
    "\n",
    "        return x_masked, mask, ids_restore\n",
    "\n",
    "    def forward_encoder(self, x, 掩码率):\n",
    "        # TODO 现在测试直接用已经分块的图像，不是真实图像再分块\n",
    "        x = self.图像块嵌入向量(x) # 图像块转化为嵌入向量 [1*361*256]\n",
    "\n",
    "        # add pos embed w/o cls token\n",
    "        x = x + self.位置嵌入向量[:, 1:, :] # 从1开始是因为0对应cls_token\n",
    "\n",
    "        # masking: length -> length * 掩码率。\n",
    "        # TODO 关键操作\n",
    "        x, mask, ids_restore = self.随机掩码(x, 掩码率)\n",
    "\n",
    "        # 添加分类嵌入向量\n",
    "        分类嵌入 = self.分类嵌入 + self.位置嵌入向量[:, :1, :]\n",
    "        分类嵌入 = 分类嵌入.expand(x.shape[0], -1, -1)\n",
    "        x = torch.cat((分类嵌入, x), dim=1)\n",
    "\n",
    "        x = self.编码块堆叠(x)\n",
    "        x = self.编码器标准化(x)\n",
    "\n",
    "        return x, mask, ids_restore\n",
    "\n",
    "    def forward_decoder(self, x, ids_restore):\n",
    "        # 编码器的输出转化为解码器的输入嵌入向量\n",
    "        x = self.解码嵌入(x)\n",
    "\n",
    "        # 加上被掩码的图像\n",
    "        mask_tokens = self.mask_token.repeat(x.shape[0], ids_restore.shape[1] + 1 - x.shape[1], 1)\n",
    "        x_ = torch.cat([x[:, 1:, :], mask_tokens], dim=1)  # no cls token\n",
    "        x_ = torch.gather(x_, dim=1, index=ids_restore.unsqueeze(-1).repeat(1, 1, x.shape[2]))  # unshuffle\n",
    "        x = torch.cat([x[:, :1, :], x_], dim=1)  # append cls token\n",
    "\n",
    "        # 添加解码器位置嵌入向量\n",
    "        x = x + self.解码器位置嵌入向量\n",
    "        x = self.解码块堆叠(x)\n",
    "        x = self.解码器标准化(x)\n",
    "        x = self.解码器预测值(x)\n",
    "\n",
    "        x = x[:, 1:, :] # 取出没有分类嵌入向量的部分\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward_loss(self, imgs, pred, mask):\n",
    "        \"\"\"\n",
    "        imgs: [批量, 3, H, W]\n",
    "        pred: [批量, L, p*p*3]\n",
    "        mask: [批量, L], 0 is keep, 1 is remove,\n",
    "        \"\"\"\n",
    "\n",
    "        target = self.patchify(imgs)\n",
    "        if self.norm_pix_loss:\n",
    "            mean = target.mean(dim=-1, keepdim=True)\n",
    "            var = target.var(dim=-1, keepdim=True)\n",
    "            target = (target - mean) / (var + 1.e-6) ** .5\n",
    "\n",
    "        loss = (pred - target) ** 2\n",
    "        loss = loss.mean(dim=-1)  # [批量, L], mean loss per patch\n",
    "\n",
    "        loss = (loss * mask).sum() / mask.sum()  # mean loss on removed patches\n",
    "        return loss\n",
    "\n",
    "    def forward(self, imgs, 掩码率=0.75):\n",
    "        # img是原始且未分割的图片\n",
    "        latent, mask, ids_restore = self.forward_encoder(imgs, 掩码率)\n",
    "        pred = self.forward_decoder(latent, ids_restore)  # [批量, L, p*p*3]\n",
    "        loss = self.forward_loss(imgs, pred, mask)\n",
    "        return loss, pred, mask"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class 直接自监督重建OCTA图像(nn.Module):\n",
    "    \"\"\"\n",
    "    现在考虑传进来的数据是已经划分块之后的结果，图像是单通道\n",
    "    \"\"\"\n",
    "    def __init__(self, 图像形状=224, 图像块大小=16, 嵌入向量维度=256, 深度=1, 注意力头数量=2, 解码器嵌入向量维度=256, 解码器深度=8,\n",
    "                 解码器注意力头数量=16, 多层感知机扩增率=4., 标准化=nn.LayerNorm, norm_pix_loss=False):\n",
    "        super().__init__()\n",
    "\n",
    "        # --------------------------------------------------------------------------\n",
    "        # MAE编码器实现\n",
    "        self.图像块嵌入向量 = 直接平面图像块嵌入(图像形状, 嵌入向量维度)\n",
    "        图像块数量 = self.图像块嵌入向量.图像块数量\n",
    "        # 第一个轴的1表示批量维度\n",
    "        self.分类嵌入 = nn.Parameter(torch.zeros(1, 1, 嵌入向量维度))\n",
    "        self.位置嵌入向量 = nn.Parameter(torch.zeros(1, 图像块数量 + 1, 嵌入向量维度),\n",
    "                                      requires_grad=False)  # fixed sin-cos embedding\n",
    "        # 作者没给transformer编码块传入dropout的丢弃率\n",
    "        self.编码块堆叠 = nn.Sequential(*[\n",
    "            编码块(特征维度=嵌入向量维度, 注意力头数量=注意力头数量, 多层感知机扩增率=多层感知机扩增率, qkv_偏差=True, 标准化=标准化)\n",
    "            for i in range(深度)])\n",
    "        self.编码器标准化 = 标准化(嵌入向量维度)\n",
    "        # --------------------------------------------------------------------------\n",
    "\n",
    "        # --------------------------------------------------------------------------\n",
    "        # MAE解码器实现\n",
    "        self.解码嵌入 = nn.Linear(嵌入向量维度, 解码器嵌入向量维度, bias=True)\n",
    "\n",
    "        # 替换被遮掩的图像块\n",
    "        self.mask_token = nn.Parameter(torch.zeros(1, 1, 解码器嵌入向量维度))\n",
    "\n",
    "        # 加1是因为解码也需要cls_token\n",
    "        self.解码器位置嵌入向量 = nn.Parameter(torch.zeros(1, 图像块数量 + 1, 解码器嵌入向量维度),\n",
    "                                              requires_grad=False)  # fixed sin-cos embedding\n",
    "\n",
    "        self.解码块堆叠 = nn.Sequential(*[\n",
    "            编码块(特征维度=解码器嵌入向量维度, 注意力头数量=解码器注意力头数量, 多层感知机扩增率=多层感知机扩增率, qkv_偏差=True, 标准化=标准化)\n",
    "            for i in range(解码器深度)])\n",
    "\n",
    "        self.解码器标准化 = 标准化(解码器嵌入向量维度)\n",
    "        # 这个就是输出时的最后一层了\n",
    "        self.解码器预测值 = nn.Linear(解码器嵌入向量维度, 图像块大小 ** 2, bias=True)  # decoder to patch\n",
    "        # --------------------------------------------------------------------------\n",
    "\n",
    "        self.norm_pix_loss = norm_pix_loss\n",
    "\n",
    "        # 初始化 (and freeze) pos_embed by sin-cos embedding\n",
    "        # 编码器的位置嵌入\n",
    "        位置嵌入向量 = get_2d_sincos_pos_embed(self.位置嵌入向量.shape[-1], int(self.图像块嵌入向量.图像块数量 ** .5),\n",
    "                                            cls_token=True)\n",
    "        self.位置嵌入向量.data.copy_(torch.from_numpy(位置嵌入向量).float().unsqueeze(0))\n",
    "\n",
    "        解码器位置嵌入向量 = get_2d_sincos_pos_embed(self.解码器位置嵌入向量.shape[-1],\n",
    "                                                    int(self.图像块嵌入向量.图像块数量 ** .5), cls_token=True)\n",
    "        self.解码器位置嵌入向量.data.copy_(torch.from_numpy(解码器位置嵌入向量).float().unsqueeze(0))\n",
    "\n",
    "        # TODO 如果在生成图像块嵌入向量的时候使用卷积操作才用到这个初始化操作\n",
    "        # initialize patch_embed like nn.Linear (instead of nn.Conv2d)\n",
    "        # 权重 = self.图像块嵌入向量.proj.weight.data\n",
    "        # torch.nn.init.xavier_uniform_(w.view([w.shape[0], -1]))\n",
    "\n",
    "        # timm's trunc_normal_(std=.02) is effectively normal_(std=0.02) as cutoff is too big (2.)\n",
    "        # torch.nn.init.normal_(self.cls_token, std=.02)\n",
    "        torch.nn.init.normal_(self.mask_token, std=.02)\n",
    "\n",
    "        # initialize nn.Linear and nn.LayerNorm\n",
    "        self.apply(self.初始化权重)\n",
    "\n",
    "    def 初始化权重(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            torch.nn.init.xavier_uniform_(m.weight)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "    # 将图片划分成块，计算损失函数的时候用到，我现在已经就是块了所以不需要\n",
    "    def patchify(self, imgs):\n",
    "        \"\"\"\n",
    "        vit中切分patch调整数据维度的操作\n",
    "        imgs: (批量, 3, H, W)\n",
    "        x: (批量, L, patch_size**2 *3)\n",
    "        \"\"\"\n",
    "        块大小 = self.图像块嵌入向量.图像块的大小 # 每个patch的长宽\n",
    "        assert imgs.shape[2] == imgs.shape[3] and imgs.shape[2] % 块大小 == 0\n",
    "\n",
    "        # 原图像的高度和宽度对块的大小做整数除法\n",
    "        高度数量 = 宽度数量 = imgs.shape[2] // 块大小  # TODO 计算patch的数量？怎么计算的\n",
    "        x = imgs.reshape(shape=(imgs.shape[0], 3, 高度数量, 块大小, 宽度数量, 块大小))\n",
    "        # batchsize，通道数，patch每一边的数量，每个patch的宽高，patch每一边的数量，每个patch的长宽\n",
    "        x = torch.einsum('nchpwq->nhwpqc', x)\n",
    "        x = x.reshape(shape=(imgs.shape[0], 高度数量 * 宽度数量, 块大小 ** 2 * 3))\n",
    "        # 其实直接转换过来就行， （batchsize，patch总数base=196，path平方x通道数也就是每个patch块内的数据）\n",
    "        return x\n",
    "\n",
    "    # 将图像块还原成完整的图像\n",
    "    def unpatchify(self, x):\n",
    "        \"\"\"\n",
    "        x: (批量, L, patch_size**2 *3)\n",
    "        imgs: (批量, 3, H, W)\n",
    "        \"\"\"\n",
    "        p = self.patch_embed.patch_size[0]\n",
    "        h = w = int(x.shape[1] ** .5)\n",
    "        assert h * w == x.shape[1]\n",
    "\n",
    "        x = x.reshape(shape=(x.shape[0], h, w, p, p, 3))\n",
    "        x = torch.einsum('nhwpqc->nchpwq', x)\n",
    "        imgs = x.reshape(shape=(x.shape[0], 3, h * p, h * p))\n",
    "        # （batchsize，patch总数base=196，path平方x通道数也就是每个patch块内的数据）->（图像宽高，通道数，hxp=patch[0]xp的宽高）\n",
    "        return imgs\n",
    "\n",
    "    # TODO 随机掩码函数，很重要\n",
    "    def 随机掩码(self, x, 掩码率):\n",
    "        \"\"\"\n",
    "        Perform per-sample random masking by per-sample shuffling.\n",
    "        Per-sample shuffling is done by argsort random noise.\n",
    "        x: [批量, L, 嵌入向量维度], sequence\n",
    "        \"\"\"\n",
    "\n",
    "        批量, 块数量, 嵌入向量维度 = x.shape  # 获取输入数据的形状，批量, 块数量, 维度\n",
    "        保留块数量 = int(块数量 * (1 - 掩码率))\n",
    "        noise = torch.rand(批量, 块数量, device=x.device)  # 二维随机噪声矩阵，数值在[0, 1]\n",
    "\n",
    "        # sort noise for each sample。argsort()返回的是元素对应的索引\n",
    "        # ids_shuffle是用来选择哪些元素做掩码\n",
    "        ids_shuffle = torch.argsort(noise, dim=1)  # 升序排列ascend: small is keep, large is remove\n",
    "        # ids_restore是用于在编码之后对图像块的顺序进行还原并输入解码器？\n",
    "        ids_restore = torch.argsort(ids_shuffle, dim=1) # 对上一步得到的索引排序\n",
    "\n",
    "        # keep the first subset\n",
    "        ids_keep = ids_shuffle[:, :保留块数量]\n",
    "        # 没有被掩码的图像块序列？\n",
    "        x_masked = torch.gather(x, dim=1, index=ids_keep.unsqueeze(-1).repeat(1, 1, 嵌入向量维度))\n",
    "\n",
    "        # generate the binary mask: 0 is keep, 1 is remove\n",
    "        #\n",
    "        mask = torch.ones([批量, 块数量], device=x.device)\n",
    "        mask[:, :保留块数量] = 0\n",
    "        # unshuffle to get the binary mask\n",
    "        mask = torch.gather(mask, dim=1, index=ids_restore) #\n",
    "\n",
    "        return x_masked, mask, ids_restore\n",
    "\n",
    "    def forward_encoder(self, x, 掩码率):\n",
    "        # TODO 现在测试直接用已经分块的图像，不是真实图像再分块\n",
    "        x = self.图像块嵌入向量(x) # 图像块转化为嵌入向量 [1*361*256]\n",
    "\n",
    "        # add pos embed w/o cls token\n",
    "        x = x + self.位置嵌入向量[:, 1:, :] # 从1开始是因为0对应cls_token\n",
    "\n",
    "        # masking: length -> length * 掩码率。\n",
    "        # TODO 关键操作\n",
    "        x, mask, ids_restore = self.随机掩码(x, 掩码率)\n",
    "\n",
    "        # 添加分类嵌入向量\n",
    "        分类嵌入 = self.分类嵌入 + self.位置嵌入向量[:, :1, :]\n",
    "        分类嵌入 = 分类嵌入.expand(x.shape[0], -1, -1)\n",
    "        x = torch.cat((分类嵌入, x), dim=1)\n",
    "\n",
    "        x = self.编码块堆叠(x)\n",
    "        x = self.编码器标准化(x)\n",
    "\n",
    "        return x, mask, ids_restore\n",
    "\n",
    "    def forward_decoder(self, x, ids_restore):\n",
    "        # 编码器的输出转化为解码器的输入嵌入向量\n",
    "        x = self.解码嵌入(x)\n",
    "\n",
    "        # 加上被掩码的图像\n",
    "        mask_tokens = self.mask_token.repeat(x.shape[0], ids_restore.shape[1] + 1 - x.shape[1], 1)\n",
    "        x_ = torch.cat([x[:, 1:, :], mask_tokens], dim=1)  # no cls token\n",
    "        x_ = torch.gather(x_, dim=1, index=ids_restore.unsqueeze(-1).repeat(1, 1, x.shape[2]))  # unshuffle\n",
    "        x = torch.cat([x[:, :1, :], x_], dim=1)  # append cls token\n",
    "\n",
    "        # 添加解码器位置嵌入向量\n",
    "        x = x + self.解码器位置嵌入向量\n",
    "        x = self.解码块堆叠(x)\n",
    "        x = self.解码器标准化(x)\n",
    "        x = self.解码器预测值(x)\n",
    "\n",
    "        x = x[:, 1:, :] # 取出没有分类嵌入向量的部分\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward_loss(self, imgs, pred, mask):\n",
    "        \"\"\"\n",
    "        imgs: [批量, 3, H, W]\n",
    "        pred: [批量, L, p*p*3]\n",
    "        mask: [批量, L], 0 is keep, 1 is remove,\n",
    "        \"\"\"\n",
    "\n",
    "        target = self.patchify(imgs)\n",
    "        if self.norm_pix_loss:\n",
    "            mean = target.mean(dim=-1, keepdim=True)\n",
    "            var = target.var(dim=-1, keepdim=True)\n",
    "            target = (target - mean) / (var + 1.e-6) ** .5\n",
    "\n",
    "        loss = (pred - target) ** 2\n",
    "        loss = loss.mean(dim=-1)  # [批量, L], mean loss per patch\n",
    "\n",
    "        loss = (loss * mask).sum() / mask.sum()  # mean loss on removed patches\n",
    "        return loss\n",
    "\n",
    "    def forward(self, imgs, 掩码率=0.75):\n",
    "        # img是原始且未分割的图片\n",
    "        latent, mask, ids_restore = self.forward_encoder(imgs, 掩码率)\n",
    "        pred = self.forward_decoder(latent, ids_restore)  # [批量, L, p*p*3]\n",
    "        loss = self.forward_loss(imgs, pred, mask)\n",
    "        return loss, pred, mask"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}